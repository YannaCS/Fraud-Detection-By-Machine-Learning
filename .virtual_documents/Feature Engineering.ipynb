


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
import pickle
import timeit
import datetime as dt
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8-deep') 


# Load the dataset
df = pd.read_csv('./data/cleaned.csv', index_col=0)
print(f"Dataset loaded: {df.shape}")

df['date'] = pd.to_datetime(df['date'])
df.zip5 = df.zip5.astype(str).str.zfill(5)
df.ssn = df.ssn.astype(str)
df.dob = df.dob.astype(str)
df.homephone = df.homephone.astype(str)

print(f"\nDataset info:")
df.info()





# Extract time features
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
#df['dow'] = df['date'].dt.dayofweek
df['dow'] = df['date'].dt.day_name()





train_test = df[df.date < '2016-11-01']


# do statistical smoothing
c = 4
nmid = 20
y_avg = train_test['fraud_label'].mean()
y_dow = train_test.groupby('dow')['fraud_label'].mean()
num = train_test.groupby('dow').size()
y_dow_smooth = y_avg + (y_dow - y_avg)/(1 + np.exp(-(num - nmid)/c))
df['dow_risk'] = df.dow.map(y_dow_smooth)


df[['year', 'month', 'day', 'dow', 'dow_risk', 'fraud_label']].sample(5)





df['name'] = df.firstname + df.lastname
df['fulladdress'] = df.address + df.zip5
df['name_dob'] = df.name + df.dob
df['name_fulladdress'] = df.name + df.fulladdress
df['name_homephone'] = df.name + df.homephone
df['fulladdress_dob'] = df.fulladdress + df.dob
df['fulladdress_homephone'] = df.fulladdress + df.homephone
df['dob_homephone'] = df.dob + df.homephone
df['homephone_name_dob'] = df.homephone + df.name_dob


for field in df.select_dtypes(include=['object', 'category']).columns:
    df['ssn_' + field] = df.ssn + df[field]
attributes = df.select_dtypes(include=['object', 'category']).columns
print(attributes)





# previous:
# start = timeit.default_timer()

# for entity in attributes:
    
#     try: print('Run time for the last entity ----------------- {}s'.format(timeit.default_timer() - st))
#     except: print('\n')
#     st = timeit.default_timer()
    
#     df_l = df1[['record', 'date', entity]]
#     df_r = df1[['check_record', 'check_date', entity]]
    
#     temp = pd.merge(df_l, df_r, left_on = entity, right_on = entity)
    
    
#     temp1 = temp[temp.record > temp.check_record][['record','date','check_date']]\
#                                                 .groupby('record')[['date', 'check_date']].last()
#     mapper = (temp1.date - temp1.check_date).dt.days
#     final[entity + '_day_since'] = final.record.map(mapper)
#     final[entity + '_day_since'].fillna((final.date - pd.to_datetime('2016-01-01')).dt.days, inplace = True)

#     print('\n' + entity + '_day_since ---> Done')
    
#     for time in [0,1,3,7,14,30]:
        
#         temp_1 = temp[(temp.check_date >= (temp.date - dt.timedelta(time))) &\
#                        (temp.record >= temp.check_record)]
        
#         col_name = entity + '_count_' + str(time)    
#         mapper2 = temp_1.groupby('record')[entity].count()      
#         final[col_name] = final.record.map(mapper2)
        
#         print(entity + '_count_' + str(time) + ' ---> Done')

# print('Total run time: {}mins'.format((timeit.default_timer() - start)/60))


df1 = df.copy()
df1['record'] = list(df1.index)
final = df.copy()
final['record'] = list(final.index)
df1['check_date'] = df1.date
df1['check_record'] = df1.record


import timeit
import datetime as dt
from typing import Dict, List
import gc





def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    """Optimize DataFrame data types to reduce memory usage."""
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != 'object':
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
    
    return df





def process_entity_chunked(df1: pd.DataFrame, entity: str, final: pd.DataFrame, 
                          chunk_size: int = 50000) -> pd.DataFrame:
    """Process entity with chunking to reduce memory usage."""
    
    print(f"\nProcessing {entity}...")
    st = timeit.default_timer()
    
    # Sort data once for efficiency
    df1_sorted = df1.sort_values('record')
    
    # Process day_since in chunks
    day_since_results = {}
    
    for start_idx in range(0, len(df1_sorted), chunk_size):
        end_idx = min(start_idx + chunk_size, len(df1_sorted))
        chunk_records = df1_sorted.iloc[start_idx:end_idx]['record'].unique()
        
        # Get relevant data for this chunk
        chunk_data = df1[df1['record'].isin(chunk_records) | 
                        df1['check_record'].isin(chunk_records)]
        
        # Self-join on entity
        df_l = chunk_data[['record', 'date', entity]]
        df_r = chunk_data[['check_record', 'check_date', entity]]
        
        temp = pd.merge(df_l, df_r, on=entity, suffixes=('', '_r'))
        temp = temp[temp.record > temp.check_record]
        
        if len(temp) > 0:
            # Calculate days since
            temp1 = temp[['record', 'date', 'check_date']].groupby('record').last()
            days_diff = (temp1['date'] - temp1['check_date']).dt.days
            
            for record, days in days_diff.items():
                day_since_results[record] = days
        
        # Clean up
        del temp, df_l, df_r
        gc.collect()
    
    # Map results
    final[entity + '_day_since'] = final['record'].map(day_since_results)
    final[entity + '_day_since'].fillna(
        (final['date'] - pd.to_datetime('2016-01-01')).dt.days, 
        inplace=True
    )
    
    print(f'{entity}_day_since ---> Done')
    
    # Process counts for different time windows
    for time_window in [0, 1, 3, 7, 14, 30]:
        count_results = {}
        
        for start_idx in range(0, len(df1_sorted), chunk_size):
            end_idx = min(start_idx + chunk_size, len(df1_sorted))
            chunk_records = df1_sorted.iloc[start_idx:end_idx]['record'].unique()
            
            # Get relevant data
            chunk_data = df1[df1['record'].isin(chunk_records) | 
                            df1['check_record'].isin(chunk_records)]
            
            # Self-join
            df_l = chunk_data[['record', 'date', entity]]
            df_r = chunk_data[['check_record', 'check_date', entity]]
            
            temp = pd.merge(df_l, df_r, on=entity, suffixes=('', '_r'))
            
            # Filter by time window
            temp_filtered = temp[
                (temp['check_date'] >= (temp['date'] - pd.Timedelta(days=time_window))) &
                (temp['record'] >= temp['check_record'])
            ]
            
            if len(temp_filtered) > 0:
                counts = temp_filtered.groupby('record')[entity].count()
                for record, count in counts.items():
                    count_results[record] = count
            
            # Clean up
            del temp, temp_filtered, df_l, df_r
            gc.collect()
        
        # Map results
        col_name = f'{entity}_count_{time_window}'
        final[col_name] = final['record'].map(count_results).fillna(0).astype('int32')
        
        print(f'{col_name} ---> Done')
    
    print(f'Entity {entity} completed in {(timeit.default_timer() - st):.2f}s')
    return final


def process_all_entities(df1: pd.DataFrame, attributes: List[str], 
                        final: pd.DataFrame, chunk_size: int = 50000) -> pd.DataFrame:
    """Process all entities with memory optimization."""
    
    start = timeit.default_timer()
    
    # Optimize data types first
    print("Optimizing data types...")
    df1 = optimize_dtypes(df1)
    
    # Ensure date columns are datetime
    df1['date'] = pd.to_datetime(df1['date'])
    df1['check_date'] = pd.to_datetime(df1['check_date'])
    
    # Process each entity
    for entity in attributes:
        final = process_entity_chunked(df1, entity, final, chunk_size)
        
        # Force garbage collection after each entity
        gc.collect()
    
    print(f'\nTotal run time: {(timeit.default_timer() - start)/60:.2f} mins')
    return final





def process_with_duckdb(df1: pd.DataFrame, attributes: List[str], 
                       final: pd.DataFrame) -> pd.DataFrame:
    """Process using DuckDB for better memory efficiency."""
    
    import duckdb
    
    start = timeit.default_timer()
    conn = duckdb.connect(':memory:')
    
    # Register DataFrame
    conn.register('df1', df1)
    conn.register('final', final)
    
    for entity in attributes:
        st = timeit.default_timer()
        
        # Calculate day_since using SQL
        query_day_since = f"""
        WITH last_records AS (
            SELECT 
                l.record,
                MAX(l.date - r.check_date) as days_diff
            FROM df1 l
            JOIN df1 r ON l.{entity} = r.{entity}
            WHERE l.record > r.check_record
            GROUP BY l.record
        )
        SELECT * FROM last_records
        """
        
        day_since_df = conn.execute(query_day_since).df()
        
        # Process each time window
        for time_window in [0, 1, 3, 7, 14, 30]:
            query_count = f"""
            SELECT 
                l.record,
                COUNT(*) as count
            FROM df1 l
            JOIN df1 r ON l.{entity} = r.{entity}
            WHERE l.record >= r.check_record
                AND r.check_date >= l.date - INTERVAL '{time_window} days'
            GROUP BY l.record
            """
            
            count_df = conn.execute(query_count).df()
            col_name = f'{entity}_count_{time_window}'
            final = final.merge(count_df, on='record', how='left')
            final.rename(columns={'count': col_name}, inplace=True)
            final[col_name].fillna(0, inplace=True)
        
        print(f'Entity {entity} completed in {(timeit.default_timer() - st):.2f}s')
    
    conn.close()
    print(f'\nTotal run time: {(timeit.default_timer() - start)/60:.2f} mins')
    return final


# final = process_all_entities(df1, attributes, final, chunk_size=25000)
# or for very large datasets:
final = process_with_duckdb(df1, attributes, final)








for att in attributes:
    for d in ['0', '1']:
        for dd in ['3', '7', '14', '30']:
            final[att + '_count_' + d + '_by_' + dd] =\
            final[att + '_count_' + d]/(final[att + '_count_' + dd]/float(dd))





final['dob'].sample(5)


final['birth_year'] = final['dob'].str[:4].astype(int)
final['birth_month'] = final['dob'].str[4:6].astype(int)
final['birth_day'] = final['dob'].str[6:8].astype(int)

# Calculate age at transaction
final['age'] = (final['date'].dt.year - final['birth_year']) + \
                               ((final['date'].dt.month - final['birth_month']) / 12)





# only keep the numerical variables
final_cut = final.copy()
for field in final_cut.select_dtypes(include=['object', 'category']).columns:
    final_cut.drop(field, inplace=True, axis=1)


final_cut.drop(['date','record'], axis=1, inplace = True)


final_cut.columns.to_list()


final_cut.shape


final_cut.sample(5)


final_cut.to_csv('./data/vars 485.csv')





data = final_cut.loc[(final_cut.index <= 830000)&(final_cut.index >= 35755)]
data['RANDOM'] = np.random.ranf(len(data))





num_vars = data.shape[1]
goods = data[data['fraud_label'] == 0]
bads = data[data['fraud_label'] == 1]

KSFDR = pd.DataFrame(np.zeros((num_vars+1,3)))
i = 0
for col in data:
        KSFDR.loc[i,0] = col
        i = i+1
KSFDR.columns=['feature','KS','FDR']


num_bads = bads.shape[0]


from scipy.stats import ks_2samp
from sklearn.feature_selection import SelectKBest





j = 0
for col in data:
    KSFDR['KS'][j] = ks_2samp(goods[col],bads[col])[0]
    j = j+1
KSFDR.sort_values(by=['KS'], ascending = False, inplace = True)
KSFDR.head()





cut = int(round(len(data)*0.03))
k = 0
for col in data:
    x1 = data.sort_values(col, ascending = False).head(cut)
    x2 = data.sort_values(col, ascending = True).head(cut)
    f1 = x1.loc[:,'fraud_label']
    f2 = x2.loc[:,'fraud_label']
    FDR1 = sum(f1)/num_bads
    FDR2 = sum(f2)/num_bads
    FDR = max(FDR1,FDR2)
    KSFDR.loc[k,'FDR'] = FDR
    k = k+1


KSFDR['rank_ks'] = KSFDR['KS'].rank(ascending = True)
KSFDR['rank_FDR'] = KSFDR['FDR'].rank(ascending = True)
KSFDR['avg_rank'] = (KSFDR['rank_ks']+KSFDR['rank_FDR'])/2
KSFDR.sort_values(by=['avg_rank'], ascending = False, inplace = True)


KSFDR.reset_index(drop=True)
num_keep = 70
cols_keep = list(KSFDR['feature'][1:num_keep])
cols_keep





newX = data.drop(['fraud_label'],axis=1).loc[:,cols_keep]
y = data.fraud_label


# wrapper
import sklearn
from sklearn.feature_selection import RFECV
from sklearn.linear_model import LogisticRegression

# step: If greater than or equal to 1, 
# then step corresponds to the (integer) number of features to remove at each iteration. 
# If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of 
# features to remove at each iteration

# cv: cross-validation generator or an iterable

# Verbose: Controls verbosity of output

# n_jobs: Number of cores to run in parallel while fitting across folds

model = LogisticRegression()
refec = RFECV(estimator = model, step=1, cv=3, verbose=3, n_jobs=-1, scoring = 'roc_auc')
refec.fit(newX, y)


plt.figure()
# Get the cross-validation scores
min_features_to_select = 1  
n_scores = len(refec.cv_results_['mean_test_score'])
plt.plot(range(min_features_to_select, n_scores + min_features_to_select), 
         refec.cv_results_['mean_test_score'])
plt.xlabel("Number of features selected")
plt.ylabel("Cross validation score (ROC AUC)")
# add the optimal number of features line
plt.axvline(x=refec.n_features_, color='r', linestyle='--', 
            label=f'Optimal number: {refec.n_features_}')
plt.legend()
plt.title("RFECV - Optimal number of features")
plt.savefig('./plots/RFECV.png', dpi=300, bbox_inches='tight')
plt.show()


selected_feature_names = newX.columns[refec.support_]
print(f"{refec.n_features_} selected feature names: {list(selected_feature_names)}")

# Show all features with their rankings
feature_ranking_df = pd.DataFrame({
    'feature': newX.columns,
    'ranking': refec.ranking_
}).sort_values('ranking')
print("\nFeature rankings:")
with pd.option_context('display.max_rows', None):
    print(feature_ranking_df)


final_cut[list(newX.columns[refec.support_])+['fraud_label']].to_csv('./data/selected.csv')





#final_cut = pd.read_csv('./data/selected.csv', index_col=0)


label = final_cut.fraud_label
label.shape


only_feature = final_cut.drop(['fraud_label'], axis=1)
print('features shape: ', only_feature.shape)
std = only_feature.std()
mean = only_feature.mean()
z_scaled = (only_feature - mean)/std

# Prevent extreme values after z-scaled
z_scaled = z_scaled.clip(lower=-1000, upper=1000)
z_scaled.describe()


z_scaled['fraud_label']=label
z_scaled.to_csv('./data/z_scaled.csv')





from sklearn.decomposition import PCA

X = z_scaled.drop(['fraud_label'], axis=1)
y = z_scaled.fraud_label
# Fit PCA with all components
pca = PCA()
pca.fit(X)
# Calculate cumulative variance explained
cumsum_var = np.cumsum(pca.explained_variance_ratio_)

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumsum_var) + 1), cumsum_var, 'bo-')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')
plt.axhline(y=0.90, color='g', linestyle='--', label='90% variance')
plt.legend()
plt.grid(True)
plt.show()
# Find number of components for desired variance
n_components_95 = round(np.argmax(cumsum_var >= 0.95))
n_components_90 = round(np.argmax(cumsum_var >= 0.90))
print(f"Components for 95% variance: {n_components_95}")
print(f"Components for 90% variance: {n_components_90}")


# Plot individual variance explained
plt.figure(figsize=(10, 6))
plt.plot(range(1, 33), pca.explained_variance_ratio_[:32], 'bo-')
plt.xlabel('Component Number')
plt.ylabel('Variance Explained')
plt.title('Scree Plot')
plt.grid(True)
plt.show()





total = X.shape[0]
X = X[0:round(total*0.8)]
y = y[0:round(total*0.8)]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)


from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

# Test different numbers of components
n_components_range = [3, 5, 7, 10]
cv_scores = []

for n in n_components_range:
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('pca', PCA(n_components=n)),
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
    
    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='roc_auc')
    cv_scores.append(scores.mean())
    print(f"n_components={n}, AUC={scores.mean():.4f} (+/- {scores.std():.4f})")

# Plot results
plt.figure(figsize=(10, 6))
plt.plot(n_components_range, cv_scores, 'bo-')
plt.xlabel('Number of PCA Components')
plt.ylabel('Cross-validated AUC Score')
plt.grid(True)
plt.show()


X = z_scaled.drop(['fraud_label'], axis=1)
y = z_scaled.fraud_label

# Apply PCA with 5 components
pca = PCA(n_components=5)
X_pca = pca.fit_transform(X)
pca_columns = [f'PC{i+1}' for i in range(5)]
df_pca = pd.DataFrame(X_pca, columns=pca_columns)
df_pca['fraud'] = y
df_pca.to_csv('./data/reduced_pca.csv')



