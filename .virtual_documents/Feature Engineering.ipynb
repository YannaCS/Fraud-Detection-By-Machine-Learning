


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
import pickle
import timeit
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8-deep') 


# Load the dataset
df = pd.read_csv('./data/cleaned.csv', index_col=0)
print(f"Dataset loaded: {df.shape}")

df['date'] = pd.to_datetime(df['date'])
df.zip5 = df.zip5.astype(str).str.zfill(5)
df.ssn = df.ssn.astype(str)
df.dob = df.dob.astype(str)
df.homephone = df.homephone.astype(str)

print(f"\nDataset info:")
df.info()


# Extract time features
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
#df['dow'] = df['date'].dt.dayofweek
df['dow'] = df['date'].dt.day_name()





train_test = df[df.date < '2016-11-01']


# do statistical smoothing
c = 4
nmid = 20
y_avg = train_test['fraud_label'].mean()
y_dow = train_test.groupby('dow')['fraud_label'].mean()
num = train_test.groupby('dow').size()
y_dow_smooth = y_avg + (y_dow - y_avg)/(1 + np.exp(-(num - nmid)/c))
df['dow_risk'] = df.dow.map(y_dow_smooth)


df[['year', 'month', 'day', 'dow', 'dow_risk', 'fraud_label']].sample(5)





df['name'] = df.firstname + df.lastname
df['fulladdress'] = df.address + df.zip5
df['name_dob'] = df.name + df.dob
df['name_fulladdress'] = df.name + df.fulladdress
df['name_homephone'] = df.name + df.homephone
df['fulladdress_dob'] = df.fulladdress + df.dob
df['fulladdress_homephone'] = df.fulladdress + df.homephone
df['dob_homephone'] = df.dob + df.homephone
df['homephone_name_dob'] = df.homephone + df.name_dob


for field in df.select_dtypes(include=['object', 'category']).columns:
    df['ssn_' + field] = df.ssn + df[field]
attributes = df.select_dtypes(include=['object', 'category']).columns
print(attributes)





import timeit
from datetime import timedelta
import gc  # for garbage collection

# Create copies of the dataframe
df1 = df.copy()
final = df.copy()

# Add required columns
df1['check_date'] = df1['date']
df1['index_id'] = df1.index 
df1['check_index_id'] = df1.index

# Start overall timer
start = timeit.default_timer()

# Initialize entity timer variable
st = None

# Process each entity in attributes
for entity in attributes:
    
    # Print runtime for previous entity (if not the first iteration)
    if st is not None:
        print('Run time for the last entity ----------------- {:.2f}s'.format(timeit.default_timer() - st))
    else:
        print('\n')
    
    # Start timer for current entity
    st = timeit.default_timer()
    
    try:
        # OPTIMIZATION 1: Filter out null values before merge
        df_l = df1[df1[entity].notna()][['index_id', 'date', entity]]
        df_r = df1[df1[entity].notna()][['check_index_id', 'check_date', entity]]
        
        # OPTIMIZATION 2: Use smaller chunks for large datasets
        if len(df_l) > 50000:
            print(f"Processing {entity} in chunks due to large size...")
            
            # Initialize result columns
            final[entity + '_day_since'] = np.nan
            for time in [0, 1, 3, 7, 14, 30]:
                final[entity + '_count_' + str(time)] = 0
            
            # Process in chunks
            chunk_size = 10000
            unique_values = df1[entity].unique()
            
            for i in range(0, len(unique_values), chunk_size):
                chunk_values = unique_values[i:i+chunk_size]
                
                # Filter dataframes for this chunk
                df_l_chunk = df_l[df_l[entity].isin(chunk_values)]
                df_r_chunk = df_r[df_r[entity].isin(chunk_values)]
                
                # Self-join on entity
                temp = pd.merge(df_l_chunk, df_r_chunk, on=entity, how='inner')
                
                # Process day_since
                temp1 = temp[temp['index_id'] > temp['check_index_id']][['index_id', 'date', 'check_date']]\
                            .groupby('index_id')[['date', 'check_date']].last()
                
                if not temp1.empty:
                    mapper = (temp1['date'] - temp1['check_date']).dt.days
                    # Update only non-null values
                    for idx, val in mapper.items():
                        if pd.isna(final.loc[idx, entity + '_day_since']):
                            final.loc[idx, entity + '_day_since'] = val
                
                # Process counts for different time windows
                for time in [0, 1, 3, 7, 14, 30]:
                    time_window = timedelta(days=time)
                    
                    temp_1 = temp[(temp['check_date'] >= (temp['date'] - time_window)) &
                                  (temp['index_id'] >= temp['check_index_id'])]
                    
                    if not temp_1.empty:
                        col_name = entity + '_count_' + str(time)
                        mapper2 = temp_1.groupby('index_id')[entity].count()
                        
                        # Add to existing counts
                        for idx, val in mapper2.items():
                            final.loc[idx, col_name] += val
                
                # Clear memory
                del temp, temp1, temp_1
                gc.collect()
            
            # Fill NaN values for day_since
            final[entity + '_day_since'].fillna(
                (final['date'] - pd.to_datetime('2016-01-01')).dt.days, 
                inplace=True
            )
            
        else:
            # Original logic for smaller datasets
            # Self-join on entity
            temp = pd.merge(df_l, df_r, on=entity, how='inner')
            
            # Find last occurrence of same entity before current index
            temp1 = temp[temp['index_id'] > temp['check_index_id']][['index_id', 'date', 'check_date']]\
                        .groupby('index_id')[['date', 'check_date']].last()
            
            # Calculate days since last occurrence
            mapper = (temp1['date'] - temp1['check_date']).dt.days
            final[entity + '_day_since'] = final.index.map(mapper)
            
            # Fill NaN values with days since 2016-01-01
            final[entity + '_day_since'].fillna(
                (final['date'] - pd.to_datetime('2016-01-01')).dt.days, 
                inplace=True
            )
            
            # Process different time windows
            for time in [0, 1, 3, 7, 14, 30]:
                time_window = timedelta(days=time)
                
                # Filter for records within time window
                temp_1 = temp[(temp['check_date'] >= (temp['date'] - time_window)) &
                              (temp['index_id'] >= temp['check_index_id'])]
                
                # Create column name and calculate counts
                col_name = entity + '_count_' + str(time)    
                mapper2 = temp_1.groupby('index_id')[entity].count()      
                final[col_name] = final.index.map(mapper2)
                
                # Fill NaN values with 0
                final[col_name].fillna(0, inplace=True)
            
            # Clear memory
            del temp, temp1
            gc.collect()
        
        print(f'\n{entity}_day_since ---> Done')
        for time in [0, 1, 3, 7, 14, 30]:
            print(f'{entity}_count_{time} ---> Done')
            
    except Exception as e:
        print(f"Error processing {entity}: {str(e)}")
        # Initialize columns with default values if error occurs
        final[entity + '_day_since'] = (final['date'] - pd.to_datetime('2016-01-01')).dt.days
        for time in [0, 1, 3, 7, 14, 30]:
            final[entity + '_count_' + str(time)] = 0
    
    # Force garbage collection after each entity
    gc.collect()

# Print total runtime
print('Total run time: {:.2f} mins'.format((timeit.default_timer() - start) / 60))





for att in attributes:
    for d in ['0', '1']:
        for dd in ['3', '7', '14', '30']:
            final[att + '_count_' + d + '_by_' + dd] =\
            final[att + '_count_' + d]/(final[att + '_count_' + dd]/float(dd))





final.set_index('record', inplace = True)


final = final.iloc[:, np.r_[8, 10, 29:337]]
final.shape


# final['fraud_label'] = df['fraud_label']


final.shape


final.head()


final.to_csv('vars 308.csv')
