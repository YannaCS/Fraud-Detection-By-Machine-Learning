from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_auc_score,roc_curve
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
import pickle #store model
import scipy
from hyperopt import fmin, tpe, hp, partial,space_eval,rand,Trials,STATUS_OK
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
%matplotlib inline

import importlib #python 3.x use reload by #importlib.reload()

import warnings
warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = (15.0,10.0) #figure size





df = pd.read_csv('z_scaled.csv')


# to create feature importance
feature_names=pd.read_csv('final_rfe.csv') # created in Feature Selection
feature_names=feature_names.feature
feature_names=feature_names[:40]
feature_names.head(5)





df.head(5)





df.drop(df.columns[0],axis=1,inplace=True) # drop column unnamed:0 to make this dataframe more pretty and efficient
df.head(5) # show the current dataframe


df.shape





label=df.fraud_label
label.shape








X=df.drop(columns=['fraud_label'])
X=X[0:833508]
X


y=label[0:833508]
y


oot=df.drop(columns=['fraud_label'])
oot=oot[833508:]
oot_label=label[833508:]
display(oot.shape)
display(oot)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)














# define a function to get the model and its auc
# auc is one of the model measures of goodness
def GBM(argsDict):
    max_depth = argsDict["max_depth"]
    n_estimators = argsDict['n_estimators']
    learning_rate = argsDict["learning_rate"]
    subsample = argsDict["subsample"] 

    global X_train,y_train

    gbm = GradientBoostingClassifier(max_depth=max_depth,  
                                     n_estimators=n_estimators,   
                                     learning_rate=learning_rate,
                                     subsample=subsample)     

    metric = cross_val_score(gbm,X_train,y_train,cv=5,scoring="roc_auc").mean()
    print(metric)
    return -metric


# define a space where Hyperopt could choose parameters by hp.choice() function
space = {"max_depth":hp.choice('max_depth',[5,6,10]),        
         "n_estimators":hp.choice('n_estimators',[100,500,800]),  
         "learning_rate":hp.choice('learning_rate',[0.02,0.05]),  
         "subsample":hp.choice('subsample',[0.1,0.5])         
        }


algo = partial(tpe.suggest,n_startup_jobs=1) 
best = fmin(GBM,space,algo=algo,max_evals=50) # cause there are total 36 choices in above space, I tried 50 evaluations


print(best)
print(GBM(best))





param={'max_depth':6,'n_estimators':1000,'learning_rate':0.02,'subsample':0.5}


from sklearn import tree
import pydotplus 
import seaborn # pictures drew by seaborn are sooooooo pretty!!!


niter = 0 #number of iteration
niter_max = 3 #maximum number of iteration
#create null dataframes to store FDR,KS,AUC,THR in each iteration
#these are all model measures of goodness
FDR = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot'])
KS = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot'])
AUC = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot'])
THR = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot']) #thresholds
ACC = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot']) #accuracy
MIS = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot']) #misclassification
FPR = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot']) #false positive rate
TPR = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot']) #true positive rate/ sensitivity/recall
TNR = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot']) #true negative rate/specificity
PRE = pd.DataFrame(np.zeros((niter_max,3)),columns=['train','test','oot']) #precision
Color=(['#CD5555','#00CED1','#FFDAB9'])





for niter in range(niter_max):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    GBDT = GradientBoostingClassifier(max_depth= param['max_depth'],
                                      learning_rate=param['learning_rate'],
                                      n_estimators= param['n_estimators'],
                                      subsample= param['subsample']).fit(X_train,y_train)
    
    prediction_train = GBDT.predict_proba(X_train)
    #Predict class probabilities for X. array with two columns,fraud=0 & fraud=1, choose fraud 1
    prediction_train = prediction_train[:,1]
    pre_train=GBDT.predict(X_train)

    prediction_test = GBDT.predict_proba(X_test)
    prediction_test = prediction_test[:,1]
    pre_test=GBDT.predict(X_test)
    
    prediction_oot = GBDT.predict_proba(oot)
    prediction_oot = prediction_oot[:,1]
    pre_oot=GBDT.predict(oot)
    X_train_copy = X_train.copy()
    X_test_copy=X_test.copy()
    oot_copy=oot.copy()
    
    #FDR
    X_train_copy['prediction'] = prediction_train
    X_train_copy['fraud label'] = y_train
    top_rows = int(round(X_train_copy.shape[0]*0.03)) #top 30%
    sorted_top_rows = X_train_copy.sort_values('prediction',ascending = False).head(top_rows)
    fdr_train = sum(sorted_top_rows['fraud label'][:])/sum(X_train_copy['fraud label'][:])
    FDR['train'][niter] = fdr_train
                                                              
    X_test_copy['prediction'] = prediction_test
    X_test_copy['fraud label'] = y_test
    top_rows = int(round(X_train_copy.shape[0]*0.03)) #top 30%
    sorted_top_rows = X_test_copy.sort_values('prediction',ascending = False).head(top_rows)
    fdr_test = sum(sorted_top_rows['fraud label'])/sum(X_test_copy['fraud label'])
    FDR['test'][niter] = fdr_test
    
    oot_copy['prediction'] = prediction_oot
    oot_copy['fraud label'] = oot_label
    top_rows = int(round(oot.shape[0]*0.03)) #top 30%
    sorted_top_rows = oot_copy.sort_values('prediction',ascending = False).head(top_rows)
    fdr_oot = sum(sorted_top_rows['fraud label'])/sum(oot_copy['fraud label'])
    FDR['oot'][niter] = fdr_oot
    
    #Confusion Matrix
    #train
    tn, fp, fn, tp = confusion_matrix(y_train, pre_train).ravel()
    CM_train=pd.DataFrame(np.zeros((2,2)),index=['Actual Good','Actual Bad'],columns=['Predicted Good','Predicted Bad'])
    CM_train.loc[:,:]=([tn,fp],[fn,tp])
    display('Confusion Matrix for train set iteration %d :'%(niter),CM_train)
    ACC['train'][niter]=(tp+tn)/(tp+tn+fp+fn)
    MIS['train'][niter]=(fp+fn)/(tp+tn+fp+fn)
    FPR['train'][niter]=fp/(fp+tn)
    TPR['train'][niter]=tp/(tp+fn)
    TNR['train'][niter]=tn/(tn+fp)
    PRE['train'][niter]=tp/(tp+fp)
    #test
    tn, fp, fn, tp = confusion_matrix(y_test, pre_test).ravel()
    CM_test=pd.DataFrame(np.zeros((2,2)),index=['Actual Good','Actual Bad'],columns=['Predicted Good','Predicted Bad'])
    CM_test.loc[:,:]=([tn,fp],[fn,tp])
    display('Confusion Matrix for test set iteration %d :'%(niter),CM_test)
    ACC['test'][niter]=(tp+tn)/(tp+tn+fp+fn)
    MIS['test'][niter]=(fp+fn)/(tp+tn+fp+fn)
    FPR['test'][niter]=fp/(fp+tn)
    TPR['test'][niter]=tp/(tp+fn)
    TNR['test'][niter]=tn/(tn+fp)
    PRE['test'][niter]=tp/(tp+fp)
    #oot
    tn, fp, fn, tp = confusion_matrix(oot_label, pre_oot).ravel()
    CM_oot=pd.DataFrame(np.zeros((2,2)),index=['Actual Good','Actual Bad'],columns=['Predicted Good','Predicted Bad'])
    CM_oot.loc[:,:]=([tn,fp],[fn,tp])
    display('Confusion Matrix for oot set iteration %d :'%(niter),CM_oot)
    ACC['oot'][niter]=(tp+tn)/(tp+tn+fp+fn)
    MIS['oot'][niter]=(fp+fn)/(tp+tn+fp+fn)
    FPR['oot'][niter]=fp/(fp+tn)
    TPR['oot'][niter]=tp/(tp+fn)
    TNR['oot'][niter]=tn/(tn+fp)
    PRE['oot'][niter]=tp/(tp+fp)
    
    #ROC/AUC
    #calculate fpr,tpr,thresholds
    fpr_train,tpr_train,thresholds_train = roc_curve(y_train,prediction_train)
    fpr_test,tpr_test,thresholds_test = roc_curve(y_test,prediction_test)
    fpr_oot, tpr_oot, thresholds_oot = roc_curve(oot_label,prediction_oot)
    #ROC curve
    plt.figure('ROC')
    plt.title('ROC curve')
    plt.plot(fpr_train,tpr_train,color=Color[niter],label='train iteration %d'%(niter),linestyle='-')
    plt.plot(fpr_test,tpr_test,color=Color[niter],label='test iteration %d'%(niter),linestyle='--')
    plt.plot(fpr_oot,tpr_oot,color=Color[niter],label='oot iteration %d'%(niter),linestyle=':')
    plt.legend(loc='lower right')
    plt.savefig('ROC_iteration_{}.png'.format(niter))
    #calculate auc
    auc_train=round(roc_auc_score(y_train,prediction_train),4)
    auc_test=round(roc_auc_score(y_test,prediction_test),4)
    auc_oot=round(roc_auc_score(oot_label,prediction_oot),4)  
    AUC['train'][niter]=auc_train
    AUC['test'][niter]=auc_test
    AUC['oot'][niter]=auc_oot
    
    #KS
    #train
    KS_max=0
    best_thr=0
    i=0
    for i in range(len(fpr_train)):
        t=tpr_train[i]
        f=fpr_train[i]
        th=thresholds_train[i]
        if (i==0):
            KS_max=t-f
            best_thr=th
        elif (t-f> KS_max):
            KS_max = t-f
            best_thr=th

    KS_max=round(KS_max,4)
    best_thr=round(best_thr,4)
    KS['train'][niter]=KS_max
    THR['train'][niter]=best_thr
    
    #test
    KS_max=0
    best_thr=0
    i=0
    for i in range(len(fpr_test)):
        t=tpr_test[i]
        f=fpr_test[i]
        th=thresholds_test[i]
        if (i==0):
            KS_max=t-f
            best_thr=th
        elif (t-f> KS_max):
            KS_max = t-f
            best_thr=th

    KS_max=round(KS_max,4)
    best_thr=round(best_thr,4)
    KS['test'][niter]=KS_max
    THR['test'][niter]=best_thr
    
    #oot
    KS_max=0
    best_thr=0
    i=0
    for i in range(len(fpr_oot)):
        t=tpr_oot[i]
        f=fpr_oot[i]
        th=thresholds_oot[i]
        if (i==0):
            KS_max=t-f
            best_thr=th
        elif (t-f> KS_max):
            KS_max = t-f
            best_thr=th

    KS_max=round(KS_max,4)
    best_thr=round(best_thr,4)
    KS['oot'][niter]=KS_max
    THR['oot'][niter]=best_thr
    
    #tree plot
    print('first GBDT Tree of iteration {}'.format(niter))
    sub_tree = GBDT.estimators_[0, 0]
    dot_data = tree.export_graphviz(
        sub_tree,
        out_file=None, filled=True,
        rounded=True,  
        special_characters=True,
        proportion=True,
        )
    graph = pydotplus.graph_from_dot_data(dot_data)  
    graph.write_png("iter_{}_tree.png".format(niter))
        
    #feature importance
    plt.figure('feat_imp')
    feat_imp=GBDT.feature_importances_
    seaborn.barplot(y=feature_names,x=feat_imp,orient='h',ci=None)
    plt.xlabel('Feature Importance')
    plt.ylabel('Features')
    plt.yticks(fontsize=8)
    plt.savefig('Feature_Importance_iter_{}.png'.format(niter))


display('FDR',FDR,'KS',KS,'AUC',AUC,'Thresholds',THR,'Accuracy',ACC,'Misclassification',MIS,'False Positive Rate',FPR,'True Positive Rate/Sensitivity/Recall',TPR,'True Negative Rate/Specificity',TNR,'Precision',PRE)


#calculate average as final goodness
GBDT_goodness=pd.DataFrame(np.zeros((10,3)),columns=['train','test','oot'],index=['FDR','KS','AUC','Thresholds','Accuracy','Misclassification','False Positive Rate','True Positive Rate','True Negative Rate','Precision'])
GBDT_goodness.loc['FDR']=[round(FDR["train"].mean(),4),round(FDR["test"].mean(),4),round(FDR["oot"].mean(),4)]
GBDT_goodness.loc['KS']=[round(KS["train"].mean(),4),round(KS["test"].mean(),4),round(KS["oot"].mean(),4)]
GBDT_goodness.loc['AUC']=[round(AUC["train"].mean(),4),round(AUC["test"].mean(),4),round(AUC["oot"].mean(),4)]
GBDT_goodness.loc['Thresholds']=[round(THR["train"].mean(),4),round(THR["test"].mean(),4),round(THR["oot"].mean(),4)]
GBDT_goodness.loc['Accuracy']=[round(ACC["train"].mean(),4),round(ACC["test"].mean(),4),round(ACC["oot"].mean(),4)]
GBDT_goodness.loc['Misclassification']=[round(MIS["train"].mean(),4),round(MIS["test"].mean(),4),round(MIS["oot"].mean(),4)]
GBDT_goodness.loc['False Positive Rate']=[round(FPR["train"].mean(),4),round(FPR["test"].mean(),4),round(FPR["oot"].mean(),4)]
GBDT_goodness.loc['True Positive Rate']=[round(TPR["train"].mean(),4),round(TPR["test"].mean(),4),round(TPR["oot"].mean(),4)]
GBDT_goodness.loc['True Negative Rate']=[round(TNR["train"].mean(),4),round(TNR["test"].mean(),4),round(TNR["oot"].mean(),4)]
GBDT_goodness.loc['Precision']=[round(PRE["train"].mean(),4),round(PRE["test"].mean(),4),round(PRE["oot"].mean(),4)]

display(GBDT_goodness)


GBDT_goodness.to_csv('D://GBDT_goodness.csv',index=True,header=True)





X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
GBDT = GradientBoostingClassifier(max_depth= param['max_depth'],
                                  learning_rate=param['learning_rate'],
                                  n_estimators= param['n_estimators'],
                                  subsample= param['subsample']).fit(X_train,y_train)


train_form=pd.DataFrame(np.zeros((20,13)),columns=['pop_bin','bin_records','bin_goods','bin_bads','bin_%good','bin_%bad','cum_records','cum_goods','cum_bads','cum_%good','cum_%bad_FDR','KS','FPR'])
test_form=pd.DataFrame(np.zeros((20,13)),columns=['pop_bin','bin_records','bin_goods','bin_bads','bin_%good','bin_%bad','cum_records','cum_goods','cum_bads','cum_%good','cum_%bad_FDR','KS','FPR'])
oot_form=pd.DataFrame(np.zeros((20,13)),columns=['pop_bin','bin_records','bin_goods','bin_bads','bin_%good','bin_%bad','cum_records','cum_goods','cum_bads','cum_%good','cum_%bad_FDR','KS','FPR'])


train_form.head(5)


display(X_train.shape,y_train.shape)
display(X_test.shape,y_test.shape)
display(oot.shape,oot_label.shape)


trn_rcd=len(y_train)
tst_rcd=len(y_test)
oot_rcd=len(oot)
trn_pre=GBDT.predict(X_train)
tst_pre=GBDT.predict(X_test)
oot_pre=GBDT.predict(oot)
trn_pre


display('train records:',trn_rcd,'test records:',tst_rcd,'oot records:',oot_rcd)


trn_bads=sum(y_train)
trn_goods=trn_rcd-trn_bads
tst_bads=sum(y_test)
tst_goods=tst_rcd-tst_bads
oot_bads=sum(oot_label)
oot_goods=oot_rcd-oot_bads
display('train goods:',trn_goods,'train bads:',trn_bads,'train goods+train bads:',trn_goods+trn_bads,'test goods:',tst_goods,'test bads:',tst_bads,'test goods+test bads:',tst_goods+tst_bads,'oot goods:',oot_goods,'oot bads:',oot_bads,'oot goods+oot bads:',oot_goods+oot_bads)


trn_fraud_rate=trn_bads/trn_rcd
tst_fraud_rate=tst_bads/tst_rcd
oot_fraud_rate=oot_bads/oot_rcd
display('train fraud rate:',trn_fraud_rate,'test fraud rate:',tst_fraud_rate,'oot fraud rate:',oot_fraud_rate)


trn_preprob=GBDT.predict_proba(X_train)[:,1]
tst_preprob=GBDT.predict_proba(X_test)[:,1]
oot_preprob=GBDT.predict_proba(oot)[:,1]


import math
trn_bin_rcd = math.ceil(trn_rcd/100)
tst_bin_rcd = math.ceil(tst_rcd/100)
oot_bin_rcd = math.ceil(oot_rcd/100)
display(trn_bin_rcd,tst_bin_rcd,oot_bin_rcd)


TRAIN=X_train.copy()
TRAIN['prediction_probability']=trn_preprob
TRAIN['label']=y_train
TEST=X_test.copy()
TEST['prediction_probability']=tst_preprob
TEST['label']=y_test
OOT=oot.copy()
OOT['prediction_probability']=oot_preprob
OOT['label']=oot_label


TRAIN=TRAIN.sort_values('prediction_probability',ascending=False)
TEST=TEST.sort_values('prediction_probability',ascending=False)
OOT=OOT.sort_values('prediction_probability',ascending=False)


for i in range(20):
    trn_top_rows=trn_bin_rcd*(i+1)
    tst_top_rows=tst_bin_rcd*(i+1)
    oot_top_rows=oot_bin_rcd*(i+1)
    #train
    train_form['pop_bin'][i]=i+1
    train_form['bin_records'][i]=trn_bin_rcd
    train_form['cum_records'][i]=trn_top_rows
    train_form['cum_bads'][i]=sum(TRAIN['label'].head(trn_top_rows))
    train_form['cum_goods'][i]=trn_top_rows-train_form['cum_bads'][i]
    train_form['cum_%good'][i]=round(train_form['cum_goods'][i]/trn_goods,4)
    train_form['cum_%bad_FDR'][i]=round(train_form['cum_bads'][i]/trn_bads,4)
    train_form['KS'][i]=(train_form['cum_%bad_FDR'][i]-train_form['cum_%good'][i])*100
    train_form['FPR'][i]=round(train_form['cum_goods'][i]/train_form['cum_bads'][i],2)
    if i==0:
        train_form['bin_goods'][i]=train_form['cum_goods'][i]
        train_form['bin_bads'][i]=train_form['cum_bads'][i]
    else:
        train_form['bin_goods'][i]=train_form['cum_goods'][i]-train_form['cum_goods'][i-1]
        train_form['bin_bads'][i]=train_form['cum_bads'][i]-train_form['cum_bads'][i-1]
    train_form['bin_%good'][i]=round(train_form['bin_goods'][i]/train_form['bin_records'][i],4)
    train_form['bin_%bad'][i]=round(train_form['bin_bads'][i]/train_form['bin_records'][i],4)
    #test
    test_form['pop_bin'][i]=i+1
    test_form['bin_records'][i]=tst_bin_rcd
    test_form['cum_records'][i]=tst_top_rows
    test_form['cum_bads'][i]=sum(TEST['label'].head(tst_top_rows))
    test_form['cum_goods'][i]=tst_top_rows-test_form['cum_bads'][i]
    test_form['cum_%good'][i]=round(test_form['cum_goods'][i]/tst_goods,4)
    test_form['cum_%bad_FDR'][i]=round(test_form['cum_bads'][i]/tst_bads,4)
    test_form['KS'][i]=(test_form['cum_%bad_FDR'][i]-test_form['cum_%good'][i])*100
    test_form['FPR'][i]=round(test_form['cum_goods'][i]/test_form['cum_bads'][i],2)
    if i==0:
        test_form['bin_goods'][i]=test_form['cum_goods'][i]
        test_form['bin_bads'][i]=test_form['cum_bads'][i]
    else:
        test_form['bin_goods'][i]=test_form['cum_goods'][i]-test_form['cum_goods'][i-1]
        test_form['bin_bads'][i]=test_form['cum_bads'][i]-test_form['cum_bads'][i-1]
    test_form['bin_%good'][i]=round(test_form['bin_goods'][i]/test_form['bin_records'][i],4)
    test_form['bin_%bad'][i]=round(test_form['bin_bads'][i]/test_form['bin_records'][i],4)
    #oot
    oot_form['pop_bin'][i]=i+1
    oot_form['bin_records'][i]=oot_bin_rcd
    oot_form['cum_records'][i]=oot_top_rows
    oot_form['cum_bads'][i]=sum(OOT['label'].head(oot_top_rows))
    oot_form['cum_goods'][i]=oot_top_rows-oot_form['cum_bads'][i]
    oot_form['cum_%good'][i]=round(oot_form['cum_goods'][i]/oot_goods,4)
    oot_form['cum_%bad_FDR'][i]=round(oot_form['cum_bads'][i]/oot_bads,4)
    oot_form['KS'][i]=(oot_form['cum_%bad_FDR'][i]-oot_form['cum_%good'][i])*100
    oot_form['FPR'][i]=round(oot_form['cum_goods'][i]/oot_form['cum_bads'][i],2)
    if i==0:
        oot_form['bin_goods'][i]=oot_form['cum_goods'][i]
        oot_form['bin_bads'][i]=oot_form['cum_bads'][i]
    else:
        oot_form['bin_goods'][i]=oot_form['cum_goods'][i]-oot_form['cum_goods'][i-1]
        oot_form['bin_bads'][i]=oot_form['cum_bads'][i]-oot_form['cum_bads'][i-1]
    oot_form['bin_%good'][i]=round(oot_form['bin_goods'][i]/oot_form['bin_records'][i],4)
    oot_form['bin_%bad'][i]=round(oot_form['bin_bads'][i]/oot_form['bin_records'][i],4)


train_form


train_form.to_csv('D://train_form.csv',index=True,header=True)


test_form


test_form.to_csv('D://test_form.csv',index=True,header=True)


oot_form


oot_form.to_csv('D://oot_form.csv',index=True,header=True)
