import pandas as pd
import numpy as np
import datetime as dt
import calendar
import timeit
start_time = pd.datetime.now()


df = pd.read_csv('applications data.csv')


df.head()


df.date = pd.to_datetime(df.date,format='%Y%m%d')


df.dtypes





#df.loc[df.ssn == 999999999, 'ssn'] = df.loc[df.ssn == 999999999, 'record']
#df.loc[df.address == '123 MAIN ST', 'address'] = df.loc[df.address == '123 MAIN ST', 'record']
#df.loc[df.homephone == 9999999999, 'homephone'] = df.loc[df.homephone == 9999999999, 'record']
#df.loc[df.dob == 19070626, 'dob'] = df.loc[df.dob == 19070626, 'record']


df.ssn = df.ssn.astype(str)
df.zip5 = df.zip5.astype(str)
df.dob = df.dob.astype(str)
df.homephone = df.homephone.astype(str)
df.address = df.address.astype(str)


# add leading 0 to zips

df['zip5'] = df.zip5.apply(lambda x: x if len(x) == 5 else '0'*(5-len(x)) + x)





df['dow'] = df.date.apply(lambda x: calendar.day_name[x.weekday()])


train_test = df[df.date < '2016-11-01']


# do statistical smoothing
c = 4; nmid = 20; y_avg = train_test['fraud_label'].mean()
y_dow = train_test.groupby('dow')['fraud_label'].mean()
num = train_test.groupby('dow').size()
y_dow_smooth = y_avg + (y_dow - y_avg)/(1 + np.exp(-(num - nmid)/c))
df['dow_risk'] = df.dow.map(y_dow_smooth)


df.head()





df['name'] = df.firstname + df.lastname
df['fulladdress'] = df.address + df.zip5
df['name_dob'] = df.name + df.dob
df['name_fulladdress'] = df.name + df.fulladdress
df['name_homephone'] = df.name + df.homephone
df['fulladdress_dob'] = df.fulladdress + df.dob
df['fulladdress_homephone'] = df.fulladdress + df.homephone
df['dob_homephone'] = df.dob + df.homephone
df['homephone_name_dob'] = df.homephone + df.name_dob


for field in list(df.iloc[:,np.r_[3:9, 12:15]].columns):
    df['ssn_' + field] = df.ssn + df[field]


attributes = list(df.iloc[:, np.r_[2, 5, 7, 8, 12:30]].columns)


attributes





df.date = pd.to_datetime(df.date)


df1 = df.copy()
final = df.copy()
df1['check_date'] = df1.date
df1['check_record'] = df1.record


import pandas as pd
import numpy as np
import timeit
from datetime import timedelta
import gc  # for garbage collection

# Create copies of the dataframe
df1 = df.copy()
final = df.copy()

# Add required columns using index instead of record
df1['check_date'] = df1['date']

# Start overall timer
start = timeit.default_timer()

# Initialize entity timer variable
st = None

# Process each entity in attributes
for entity in attributes:
    
    # Print runtime for previous entity
    if st is not None:
        print('Run time for the last entity ----------------- {:.2f}s'.format(timeit.default_timer() - st))
    else:
        print('\n')
    
    # Start timer for current entity
    st = timeit.default_timer()
    
    try:
        # Create dataframes with index for merge
        # Reset index to make it a column for merging
        df_l = df1[['date', entity]].reset_index()
        df_r = df1[['check_date', entity]].reset_index()
        df_r.rename(columns={'index': 'check_index'}, inplace=True)
        
        # Filter out null values to reduce memory usage
        df_l = df_l[df_l[entity].notna()]
        df_r = df_r[df_r[entity].notna()]
        
        # Check if we need chunked processing
        if len(df_l) > 50000:
            print(f"Processing {entity} in chunks due to large size...")
            
            # Initialize result columns
            final[entity + '_day_since'] = np.nan
            for time in [0, 1, 3, 7, 14, 30]:
                final[entity + '_count_' + str(time)] = 0
            
            # Process in chunks
            chunk_size = 10000
            unique_values = df1[entity].dropna().unique()
            
            for i in range(0, len(unique_values), chunk_size):
                chunk_values = unique_values[i:i+chunk_size]
                
                # Filter dataframes for this chunk
                df_l_chunk = df_l[df_l[entity].isin(chunk_values)]
                df_r_chunk = df_r[df_r[entity].isin(chunk_values)]
                
                # Self-join on entity
                temp = pd.merge(df_l_chunk, df_r_chunk, on=entity, how='inner')
                
                # Find last occurrence before current index
                temp1 = temp[temp['index'] > temp['check_index']][['index', 'date', 'check_date']]\
                            .groupby('index')[['date', 'check_date']].last()
                
                if not temp1.empty:
                    mapper = (temp1['date'] - temp1['check_date']).dt.days
                    # Update only non-null values
                    for idx, val in mapper.items():
                        if pd.isna(final.loc[idx, entity + '_day_since']):
                            final.loc[idx, entity + '_day_since'] = val
                
                # Process counts for different time windows
                for time in [0, 1, 3, 7, 14, 30]:
                    time_window = timedelta(days=time)
                    
                    temp_1 = temp[(temp['check_date'] >= (temp['date'] - time_window)) &
                                  (temp['index'] >= temp['check_index'])]
                    
                    if not temp_1.empty:
                        col_name = entity + '_count_' + str(time)
                        mapper2 = temp_1.groupby('index')[entity].count()
                        
                        # Add to existing counts
                        for idx, val in mapper2.items():
                            final.loc[idx, col_name] += val
                
                # Clear memory
                del temp, temp1, temp_1
                gc.collect()
            
            # Fill NaN values for day_since
            final[entity + '_day_since'].fillna(
                (final['date'] - pd.to_datetime('2016-01-01')).dt.days, 
                inplace=True
            )
            
        else:
            # Original logic for smaller datasets
            # Self-join on entity
            temp = pd.merge(df_l, df_r, on=entity, how='inner')
            
            # Find last occurrence before current index
            temp1 = temp[temp['index'] > temp['check_index']][['index', 'date', 'check_date']]\
                        .groupby('index')[['date', 'check_date']].last()
            
            # Calculate days since last occurrence
            mapper = (temp1['date'] - temp1['check_date']).dt.days
            final[entity + '_day_since'] = mapper
            
            # Fill NaN values with days since 2016-01-01
            final[entity + '_day_since'].fillna(
                (final['date'] - pd.to_datetime('2016-01-01')).dt.days, 
                inplace=True
            )
            
            # Process different time windows
            for time in [0, 1, 3, 7, 14, 30]:
                time_window = timedelta(days=time)
                
                # Filter for records within time window
                temp_1 = temp[(temp['check_date'] >= (temp['date'] - time_window)) &
                              (temp['index'] >= temp['check_index'])]
                
                # Create column name and calculate counts
                col_name = entity + '_count_' + str(time)    
                mapper2 = temp_1.groupby('index')[entity].count()
                final[col_name] = mapper2
                
                # Fill NaN values with 0
                final[col_name].fillna(0, inplace=True)
            
            # Clear memory
            del temp, temp1
            gc.collect()
        
        print(f'\n{entity}_day_since ---> Done')
        for time in [0, 1, 3, 7, 14, 30]:
            print(f'{entity}_count_{time} ---> Done')
            
    except Exception as e:
        print(f"Error processing {entity}: {str(e)}")
        # Initialize columns with default values if error occurs
        final[entity + '_day_since'] = (final['date'] - pd.to_datetime('2016-01-01')).dt.days
        for time in [0, 1, 3, 7, 14, 30]:
            final[entity + '_count_' + str(time)] = 0
    
    # Force garbage collection after each entity
    gc.collect()

# Print total runtime
print('Total run time: {:.2f} mins'.format((timeit.default_timer() - start) / 60))





start = timeit.default_timer()

for att in attributes:
    for d in ['0', '1']:
        for dd in ['3', '7', '14', '30']:
            final[att + '_count_' + d + '_by_' + dd] =\
            final[att + '_count_' + d]/(final[att + '_count_' + dd]/float(dd))

print('Total run time: {}s'.format(timeit.default_timer() - start))





final.set_index('record', inplace = True)


final = final.iloc[:, np.r_[8, 10, 29:337]]
final.shape


# final['fraud_label'] = df['fraud_label']


final.shape


final.head()


final.to_csv('vars 308.csv')


print('Duration: ', pd.datetime.now() - start_time)
