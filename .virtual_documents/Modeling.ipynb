


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import time
import pickle
import warnings
warnings.filterwarnings('ignore')

# Model imports
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Model selection and evaluation
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                           roc_auc_score, roc_curve, precision_recall_curve, 
                           confusion_matrix, classification_report, average_precision_score)
# self-definced functions
from func import optimize_dtypes





z_scaled = pd.read_csv('./data/z_scaled.csv', index_col=0)
print('dataset shape: ', z_scaled.shape)
print('5 sample:')
display(z_scaled.sample(5))
z_scaled = optimize_dtypes(z_scaled)





total = z_scaled.shape[0]
X = z_scaled.drop(['fraud_label'], axis=1)
y = z_scaled.fraud_label
X_oot = X[round(total*0.8):]
y_oot = y[round(total*0.8):]
X = X[0:round(total*0.8)]
y = y[0:round(total*0.8)]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
print(f"Train set loaded: {X_train.shape}")
print(f"Test set loaded: {X_test.shape}")
print(f"OOT set loaded: {X_oot.shape}")
print("check length sum euqals to original length: ",len(y_train)+len(y_test)+len(y_oot)==len(z_scaled.fraud_label))
print(f"Fraud rate in train: {y_train.mean()*100:.2f}%")


del z_scaled








baseline_models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    #'SVM': SVC(probability=True, random_state=42),
    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss'),
    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbosity=-1)
}


baseline_results = {}

for name, model in baseline_models.items():
    print(f"\nTraining {name}...")
    start_time = time.time()
    
    # Train model
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Calculate metrics
    baseline_results[name] = {
        'model': model,
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'roc_auc': roc_auc_score(y_test, y_pred_proba),
        'avg_precision': average_precision_score(y_test, y_pred_proba),
        'train_time': time.time() - start_time
    }
    
    print(f"ROC-AUC: {baseline_results[name]['roc_auc']:.4f}")
    print(f"F1 Score: {baseline_results[name]['f1']:.4f}")
    print(f"Training time: {baseline_results[name]['train_time']:.2f} seconds")

# Display baseline results
baseline_df = pd.DataFrame(baseline_results).T
print("\nBaseline Model Comparison:")
print(baseline_df.round(4))


baseline_df.to_csv('Baseline Model Performance.csv')





baseline_df = pd.read_csv('Baseline Model Performance.csv', index_col=0)
#baseline_df


fraud_weights = {
        'recall': 0.35,       # Most important - catch fraud cases
        'f1': 0.25,           # Balance of precision and recall
        'precision': 0.15,    # Avoid too many false alarms
        'roc_auc': 0.15,      # Overall discrimination ability
        'avg_precision': 0.10 # Performance across thresholds
    }


from func import plot_model_comparison
plot_model_comparison(baseline_df, weights = fraud_weights, top_n=3, 
                      figsize=(20, 20), figsave=True, figtitle = './plots/Baseline Models Comparison')


from func import select_best_fraud_models
top_models, weighted_score = select_best_fraud_models(baseline_df, top_n=3, weights=fraud_weights)


top_models


weighted_score





top_models = top_models + ['Gradient Boosting']





top_models.remove('Neural Network')
top_models








from func_2stageOpt import two_stage_optimize_multiple
optimized_models = two_stage_optimize_multiple(
    X_train, y_train,
    models = top_models,
    n_trials = 150,   # 150 trials per model
    stage1_ratio=0.3,  # 30% for exploration
    show_trial_details=False  # Clean output
)


# and less trials for NN
from func_2stageOpt import two_stage_optimization
optimized_NN = two_stage_optimization(
    X_train, y_train,
    model_name='Neural Network',
    stage1_trials=15,  # for exploration
    stage2_trials=30,  # for exploitation
    range_factor=0.5,  # Search Â±50% around best values
    verbose=True,
    show_trial_details=False  # Clean output
)


optimized_models


pd.DataFrame(optimized_models).to_csv('Optimized Results.csv')


fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12, 6))
i = 0
for name, model in optimized_models.items():
    ax[0, i].plot(model['stage2_history'])
    ax[0, i].set_xlabel('Trial')
    ax[0, i].set_ylabel('Best Score = 100% * F1')
    if i == 1:
        title = f'stage 1: RandomizedSearch\n{name}'
    else:
        title = f'\n{name}'
    ax[0, i].set_title(title)
    
    ax[1, i].plot(model['stage2_history'])
    ax[1, i].set_xlabel('Trial')
    ax[1, i].set_ylabel('Best Score = 60% * Recall + 40% * F1')
    if i == 1:
        title = f'stage 2: optuna\n{name}'
    else:
        title = f'\n{name}'
    ax[1, i].set_title(title)
    i += 1
plt.suptitle('Optimization Progress')
plt.tight_layout()
plt.show()











# optimized_models = pd.read_csv('Top3 Optimized Results.csv', index_col=0)
# optimized_models
# Convert string to actual model objects
# optimized_models = {name: eval(model_str) for name, model_str in dict(optimized_models.loc['model',:]).items()}


optimized_results = {}

for name, model in optimized_models.items():
    model = model['model']
    print(f"\nTraining {name}...")
    start_time = time.time()
    
    # Train model
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Calculate metrics
    optimized_results[name] = {
        'model': model,
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'roc_auc': roc_auc_score(y_test, y_pred_proba),
        'avg_precision': average_precision_score(y_test, y_pred_proba),
        'train_time': time.time() - start_time
    }
    
    print(f"ROC-AUC: {optimized_results[name]['roc_auc']:.4f}")
    print(f"F1 Score: {optimized_results[name]['f1']:.4f}")
    print(f"Training time: {optimized_results[name]['train_time']:.2f} seconds")

# Display optimized models results
optimized_df = pd.DataFrame(optimized_results).T
print("\nOptimized Model Comparison:")
print(optimized_df.round(4))


optimized_df.to_csv('Optimized Model Performance.csv')








baseline_df.loc[optimized_df.index, :]


optimized_df


from func import compare_performance
baseline_df = pd.read_csv('Baseline Model Performance.csv', index_col=0)
#optimized_df = pd.read_csv('Optimized Model Performance.csv', index_col=0)
compare_performance(baseline_df, optimized_df
                    , save_path = './plots/Optimization Effect.png'
                   )











#optimized_df = pd.read_csv('Optimized Model Performance.csv', index_col=0)


fraud_weights = {
        'recall': 0.35,       # Most important - catch fraud cases
        'f1': 0.25,           # Balance of precision and recall
        'precision': 0.15,    # Avoid too many false alarms
        'roc_auc': 0.15,      # Overall discrimination ability
        'avg_precision': 0.10 # Performance across thresholds
    }


from func import select_best_fraud_models
best_model_name, weighted_score = select_best_fraud_models(optimized_df, top_n=1, weights=fraud_weights)


from func import plot_model_comparison
plot_model_comparison(optimized_df, weights = fraud_weights, top_n=1, 
                      figsize=(18, 18), figsave=True, figtitle = 'Optimized Model Comparison')








# optimized_models = pd.read_csv('Top3 Optimized Results.csv', index_col=0)
# Convert string to actual model objects if read from csv
# optimized_models = {name: eval(model_str) for name, model_str in dict(optimized_models.loc['model',:]).items()}
best_model = optimized_models[best_model_name[0]]['model']
best_model


z_scaled = pd.read_csv('./data/z_scaled.csv', index_col=0)
z_scaled = optimize_dtypes(z_scaled)


# Evaluate model
from func import evaluate_model_goodness
goodness_summary, detailed_metrics = evaluate_model_goodness(
    model=best_model, model_name=best_model_name[0],
    whole_dataset = z_scaled, test_size=0.2, 
    top_percent=0.03, niter_max=5,
    display_confusion_matrices=False
)








from func import calculate_performance_forms, plot_performance_metrics
forms_dict = calculate_performance_forms(
    model=best_model,
    whole_dataset=z_scaled,
    n_bins=20,
    model_name=best_model_name[0]
)


plot_performance_metrics(forms_dict, model_name=best_model_name[0])


# train_form = forms_dict['train_form']
# test_form = forms_dict['test_form']
oot_form = forms_dict['oot_form']
oot_form



