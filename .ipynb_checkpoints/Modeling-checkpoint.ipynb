{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bd356c-08e9-4ea6-93aa-6b5cd1a3d60a",
   "metadata": {},
   "source": [
    "in this part, first models will be built and compared on the the z_scaled dataset, which has been cleaned, applied feature selection, and standardized, containing 32 features (excluding fraud label).\n",
    "\n",
    "then, the best model will be built on the dataset that has been reduced to 5  component (the best and least to explain 95% variance) by pca. The results on two dataset will be compared.\n",
    "\n",
    "at last, the best model will be applied on the best dataset's out of time (oot) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1077702d-eb4e-41e9-aceb-dea8fd785175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Model selection and evaluation\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                           confusion_matrix, classification_report, average_precision_score)\n",
    "# self-definced functions\n",
    "from func import optimize_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ad3a2-57e8-4543-a25a-96fb7d4408e6",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eea4ac9-d3d3-4b86-9bb2-01d04659fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (981694, 39)\n",
      "5 sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fulladdress_count_30</th>\n",
       "      <th>address_count_14</th>\n",
       "      <th>fulladdress_count_14</th>\n",
       "      <th>address_count_7</th>\n",
       "      <th>fulladdress_count_7</th>\n",
       "      <th>fulladdress_count_0_by_7</th>\n",
       "      <th>address_count_1</th>\n",
       "      <th>fulladdress_count_1</th>\n",
       "      <th>ssn_dob_count_30</th>\n",
       "      <th>ssn_count_30</th>\n",
       "      <th>...</th>\n",
       "      <th>fulladdress_homephone_count_7</th>\n",
       "      <th>fulladdress_homephone_count_0_by_30</th>\n",
       "      <th>ssn_dob_count_0_by_14</th>\n",
       "      <th>ssn_ssn_count_0_by_30</th>\n",
       "      <th>ssn_firstname_count_0_by_14</th>\n",
       "      <th>ssn_ssn_count_7</th>\n",
       "      <th>ssn_lastname_count_0_by_30</th>\n",
       "      <th>ssn_name_count_0_by_30</th>\n",
       "      <th>name_dob_count_0_by_30</th>\n",
       "      <th>fraud_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>910361</th>\n",
       "      <td>-0.10148</td>\n",
       "      <td>-0.088118</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.072244</td>\n",
       "      <td>-0.066696</td>\n",
       "      <td>0.102735</td>\n",
       "      <td>-0.050525</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>-0.09749</td>\n",
       "      <td>-0.098988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057383</td>\n",
       "      <td>0.171797</td>\n",
       "      <td>0.124092</td>\n",
       "      <td>0.174492</td>\n",
       "      <td>0.124638</td>\n",
       "      <td>-0.057567</td>\n",
       "      <td>0.173035</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.177126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884128</th>\n",
       "      <td>-0.10148</td>\n",
       "      <td>-0.088118</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.072244</td>\n",
       "      <td>-0.066696</td>\n",
       "      <td>0.102735</td>\n",
       "      <td>-0.050525</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>-0.09749</td>\n",
       "      <td>-0.098988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057383</td>\n",
       "      <td>0.171797</td>\n",
       "      <td>0.124092</td>\n",
       "      <td>0.174492</td>\n",
       "      <td>0.124638</td>\n",
       "      <td>-0.057567</td>\n",
       "      <td>0.173035</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.177126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101151</th>\n",
       "      <td>-0.10148</td>\n",
       "      <td>-0.088118</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.072244</td>\n",
       "      <td>-0.066696</td>\n",
       "      <td>0.102735</td>\n",
       "      <td>-0.050525</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>-0.09749</td>\n",
       "      <td>-0.098988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057383</td>\n",
       "      <td>0.171797</td>\n",
       "      <td>0.124092</td>\n",
       "      <td>0.174492</td>\n",
       "      <td>0.124638</td>\n",
       "      <td>-0.057567</td>\n",
       "      <td>0.173035</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.177126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430771</th>\n",
       "      <td>-0.10148</td>\n",
       "      <td>-0.088118</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.072244</td>\n",
       "      <td>-0.066696</td>\n",
       "      <td>0.102735</td>\n",
       "      <td>-0.050525</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>-0.09749</td>\n",
       "      <td>-0.098988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057383</td>\n",
       "      <td>0.171797</td>\n",
       "      <td>0.124092</td>\n",
       "      <td>0.174492</td>\n",
       "      <td>0.124638</td>\n",
       "      <td>-0.057567</td>\n",
       "      <td>0.173035</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.177126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792180</th>\n",
       "      <td>-0.10148</td>\n",
       "      <td>-0.088118</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.072244</td>\n",
       "      <td>-0.066696</td>\n",
       "      <td>0.102735</td>\n",
       "      <td>-0.050525</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>-0.09749</td>\n",
       "      <td>-0.098988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057383</td>\n",
       "      <td>0.171797</td>\n",
       "      <td>0.124092</td>\n",
       "      <td>0.174492</td>\n",
       "      <td>0.124638</td>\n",
       "      <td>-0.057567</td>\n",
       "      <td>0.173035</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.177126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fulladdress_count_30  address_count_14  fulladdress_count_14  \\\n",
       "910361              -0.10148         -0.088118             -0.078613   \n",
       "884128              -0.10148         -0.088118             -0.078613   \n",
       "101151              -0.10148         -0.088118             -0.078613   \n",
       "430771              -0.10148         -0.088118             -0.078613   \n",
       "792180              -0.10148         -0.088118             -0.078613   \n",
       "\n",
       "        address_count_7  fulladdress_count_7  fulladdress_count_0_by_7  \\\n",
       "910361        -0.072244            -0.066696                  0.102735   \n",
       "884128        -0.072244            -0.066696                  0.102735   \n",
       "101151        -0.072244            -0.066696                  0.102735   \n",
       "430771        -0.072244            -0.066696                  0.102735   \n",
       "792180        -0.072244            -0.066696                  0.102735   \n",
       "\n",
       "        address_count_1  fulladdress_count_1  ssn_dob_count_30  ssn_count_30  \\\n",
       "910361        -0.050525            -0.049118          -0.09749     -0.098988   \n",
       "884128        -0.050525            -0.049118          -0.09749     -0.098988   \n",
       "101151        -0.050525            -0.049118          -0.09749     -0.098988   \n",
       "430771        -0.050525            -0.049118          -0.09749     -0.098988   \n",
       "792180        -0.050525            -0.049118          -0.09749     -0.098988   \n",
       "\n",
       "        ...  fulladdress_homephone_count_7  \\\n",
       "910361  ...                      -0.057383   \n",
       "884128  ...                      -0.057383   \n",
       "101151  ...                      -0.057383   \n",
       "430771  ...                      -0.057383   \n",
       "792180  ...                      -0.057383   \n",
       "\n",
       "        fulladdress_homephone_count_0_by_30  ssn_dob_count_0_by_14  \\\n",
       "910361                             0.171797               0.124092   \n",
       "884128                             0.171797               0.124092   \n",
       "101151                             0.171797               0.124092   \n",
       "430771                             0.171797               0.124092   \n",
       "792180                             0.171797               0.124092   \n",
       "\n",
       "        ssn_ssn_count_0_by_30  ssn_firstname_count_0_by_14  ssn_ssn_count_7  \\\n",
       "910361               0.174492                     0.124638        -0.057567   \n",
       "884128               0.174492                     0.124638        -0.057567   \n",
       "101151               0.174492                     0.124638        -0.057567   \n",
       "430771               0.174492                     0.124638        -0.057567   \n",
       "792180               0.174492                     0.124638        -0.057567   \n",
       "\n",
       "        ssn_lastname_count_0_by_30  ssn_name_count_0_by_30  \\\n",
       "910361                    0.173035                  0.1727   \n",
       "884128                    0.173035                  0.1727   \n",
       "101151                    0.173035                  0.1727   \n",
       "430771                    0.173035                  0.1727   \n",
       "792180                    0.173035                  0.1727   \n",
       "\n",
       "        name_dob_count_0_by_30  fraud_label  \n",
       "910361                0.177126            0  \n",
       "884128                0.177126            0  \n",
       "101151                0.177126            0  \n",
       "430771                0.177126            0  \n",
       "792180                0.177126            0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OPTIMIZING DATA TYPES\n",
      "----------------------------------------\n",
      "Starting memory usage: 299.59 MB\n",
      "Ending memory usage: 150.73 MB\n",
      "Memory reduction: 49.7%\n"
     ]
    }
   ],
   "source": [
    "z_scaled = pd.read_csv('./data/z_scaled.csv', index_col=0)\n",
    "print('dataset shape: ', z_scaled.shape)\n",
    "print('5 sample:')\n",
    "display(z_scaled.sample(5))\n",
    "z_scaled = optimize_dtypes(z_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a75b678-2087-4e17-9a33-84953bd55dff",
   "metadata": {},
   "source": [
    "## split statistics\n",
    "- 80% train, test set (X, y will be divided by train_test_split function) and\n",
    "- 20% Out Of Time (OOT) set for validation of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80068058-407e-4f14-a8ef-429c2a244bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set loaded: (549748, 38)\n",
      "Test set loaded: (235607, 38)\n",
      "OOT set loaded: (196339, 38)\n",
      "check length sum euqals to original length:  True\n",
      "Fraud rate in train: 1.45%\n"
     ]
    }
   ],
   "source": [
    "total = z_scaled.shape[0]\n",
    "X = z_scaled.drop(['fraud_label'], axis=1)\n",
    "y = z_scaled.fraud_label\n",
    "X_oot = X[round(total*0.8):]\n",
    "y_oot = y[round(total*0.8):]\n",
    "X = X[0:round(total*0.8)]\n",
    "y = y[0:round(total*0.8)]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(f\"Train set loaded: {X_train.shape}\")\n",
    "print(f\"Test set loaded: {X_test.shape}\")\n",
    "print(f\"OOT set loaded: {X_oot.shape}\")\n",
    "print(\"check length sum euqals to original length: \",len(y_train)+len(y_test)+len(y_oot)==len(z_scaled.fraud_label))\n",
    "print(f\"Fraud rate in train: {y_train.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e72173-d121-4dd3-b1cc-225c2c0338dc",
   "metadata": {},
   "source": [
    "# Build and Compare Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfda107-e3e2-4c7c-ad0b-733d90b59b57",
   "metadata": {},
   "source": [
    "traditional svm works really slow for this huge dataset, and from the previous research, it didn't show any better than GBDT, so just skip it here. \n",
    "- have not tried replacing it with its approximations, such as LinearSVC and SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4c2695-488a-46c7-bc87-ae8073a20fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "ROC-AUC: 0.7666\n",
      "F1 Score: 0.5578\n",
      "Training time: 1.35 seconds\n",
      "\n",
      "Training Decision Tree...\n",
      "ROC-AUC: 0.7252\n",
      "F1 Score: 0.5873\n",
      "Training time: 1.09 seconds\n",
      "\n",
      "Training Random Forest...\n",
      "ROC-AUC: 0.7650\n",
      "F1 Score: 0.5978\n",
      "Training time: 4.64 seconds\n",
      "\n",
      "Training Gradient Boosting...\n",
      "ROC-AUC: 0.7644\n",
      "F1 Score: 0.6100\n",
      "Training time: 17.76 seconds\n",
      "\n",
      "Training Neural Network...\n",
      "ROC-AUC: 0.7614\n",
      "F1 Score: 0.6133\n",
      "Training time: 35.02 seconds\n",
      "\n",
      "Training XGBoost...\n",
      "ROC-AUC: 0.7679\n",
      "F1 Score: 0.6015\n",
      "Training time: 1.52 seconds\n",
      "\n",
      "Training LightGBM...\n",
      "ROC-AUC: 0.7678\n",
      "F1 Score: 0.6065\n",
      "Training time: 1.69 seconds\n",
      "\n",
      "Baseline Model Comparison:\n",
      "                                                                 model  \\\n",
      "Logistic Regression  LogisticRegression(max_iter=1000, random_state...   \n",
      "Decision Tree                  DecisionTreeClassifier(random_state=42)   \n",
      "Random Forest        (DecisionTreeClassifier(max_features='sqrt', r...   \n",
      "Gradient Boosting    ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
      "Neural Network       MLPClassifier(hidden_layer_sizes=(100, 50), ma...   \n",
      "XGBoost              XGBClassifier(base_score=None, booster=None, c...   \n",
      "LightGBM                 LGBMClassifier(random_state=42, verbosity=-1)   \n",
      "\n",
      "                     accuracy precision    recall        f1   roc_auc  \\\n",
      "Logistic Regression  0.990735  0.894737  0.405238  0.557829  0.766642   \n",
      "Decision Tree        0.990993  0.865826  0.444379   0.58732  0.725172   \n",
      "Random Forest        0.991121  0.861973  0.457622  0.597847  0.765027   \n",
      "Gradient Boosting    0.991274  0.858058   0.47322  0.610015  0.764372   \n",
      "Neural Network       0.991282  0.851097    0.4794  0.613328  0.761351   \n",
      "XGBoost              0.991142  0.856444  0.463508  0.601489  0.767878   \n",
      "LightGBM             0.991244  0.861789  0.467922  0.606523  0.767754   \n",
      "\n",
      "                    avg_precision train_time  \n",
      "Logistic Regression      0.495258   1.346485  \n",
      "Decision Tree            0.444922   1.090112  \n",
      "Random Forest            0.508431   4.640645  \n",
      "Gradient Boosting        0.511697  17.764161  \n",
      "Neural Network           0.511903  35.017555  \n",
      "XGBoost                  0.516353   1.522047  \n",
      "LightGBM                 0.515726    1.68863  \n"
     ]
    }
   ],
   "source": [
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    #'SVM': SVC(probability=True, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbosity=-1)\n",
    "}\n",
    "\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    baseline_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'avg_precision': average_precision_score(y_test, y_pred_proba),\n",
    "        'train_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    print(f\"ROC-AUC: {baseline_results[name]['roc_auc']:.4f}\")\n",
    "    print(f\"F1 Score: {baseline_results[name]['f1']:.4f}\")\n",
    "    print(f\"Training time: {baseline_results[name]['train_time']:.2f} seconds\")\n",
    "\n",
    "# Display baseline results\n",
    "baseline_df = pd.DataFrame(baseline_results).T\n",
    "print(\"\\nBaseline Model Comparison:\")\n",
    "print(baseline_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a688a-e0b0-4a7c-bfc2-bd6e2dc4858b",
   "metadata": {},
   "source": [
    "For Fraud Detection, Recall is crucial that we want to catch as many fraud cases as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dabcd7-2ce2-4281-8c6f-9feaa5009de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from func import select_best_fraud_models\n",
    "fraud_weights = {\n",
    "        'recall': 0.35,       # Most important - catch fraud cases\n",
    "        'f1': 0.25,           # Balance of precision and recall\n",
    "        'precision': 0.15,    # Avoid too many false alarms\n",
    "        'roc_auc': 0.15,      # Overall discrimination ability\n",
    "        'avg_precision': 0.10 # Performance across thresholds\n",
    "    }\n",
    "    best_models_fraud = select_best_fraud_models(baseline_df, top_n=3, weights=fraud_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7377d24-c452-46ed-9a79-cb876427e51a",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "Select top 5 models for optimization based on baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ecb6d3-9690-4f47-9ffa-43963d431d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = ['Random Forest', 'Gradient Boosting', 'Neural Network', 'XGBoost', 'LightGBM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3149a6d1-714f-41cc-bcbe-fa368f714bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 500, 800],\n",
    "        'max_depth': [5, 6, 10, 15],\n",
    "        'min_samples_split': [10, 20],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "        'class_weight': ['balanced', {0: 1, 1: 100}]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 500, 800],\n",
    "        'max_depth': [5, 6, 10],\n",
    "        'learning_rate': [0.01, 0.02, 0.05],\n",
    "        'subsample': [0.2, 0.5, 0.8],\n",
    "        'min_samples_split': [10, 20]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 500, 800],\n",
    "        'max_depth': [5, 6, 10],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "        'subsample': [0.2, 0.5, 0.8],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'scale_pos_weight': [50, 100, 200]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 500, 800],\n",
    "        'max_depth': [4, 6, 8],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'feature_fraction': [0.8, 1.0],\n",
    "        'bagging_fraction': [0.8, 1.0],\n",
    "        'bagging_freq': [5, 10]\n",
    "    },\n",
    "    'Neural Network': {\n",
    "        'hidden_layer_sizes': [(100,), (100, 50), (100, 50, 25)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "}\n",
    "\n",
    "optimized_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8cfc3d-6773-41a4-b11d-701e58562387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Processing Multiple Models\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Define the optimization function that will run in parallel\n",
    "def optimize_model(model_name, X_train, y_train, param_grid):\n",
    "    print(f\"\\nStarting optimization for {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get base model\n",
    "    if model_name == 'Random Forest':\n",
    "        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        base_model = GradientBoostingClassifier(random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        base_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    elif model_name == 'LightGBM':\n",
    "        base_model = LGBMClassifier(random_state=42, verbosity=-1, is_unbalance=True)\n",
    "    elif model_name == 'Neural Network':\n",
    "        base_model = MLPClassifier(max_iter=1000, random_state=42)\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        base_model,\n",
    "        param_grid,\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=0  # Set to 0 for cleaner parallel output\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate time taken\n",
    "    time_taken = time.time() - start_time\n",
    "    \n",
    "    print(f\"Completed {model_name} in {time_taken:.2f} seconds\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV ROC-AUC: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return model_name, {\n",
    "        'model': grid_search.best_estimator_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_score': grid_search.best_score_,\n",
    "        'time_taken': time_taken\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b85555-ab91-40e6-aea8-e010c30b6349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel optimization...\n",
      "\n",
      "Starting optimization for LightGBM...\n",
      "\n",
      "Starting optimization for XGBoost...\n",
      "\n",
      "Starting optimization for Neural Network...\n",
      "\n",
      "Starting optimization for Random Forest...\n",
      "\n",
      "Starting optimization for Gradient Boosting...\n",
      "Completed Random Forest in 4792.98 seconds\n",
      "Best parameters: {'class_weight': {0: 1, 1: 100}, 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "Best CV ROC-AUC: 0.7724\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting parallel optimization...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m overall_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(\n\u001b[1;32m      5\u001b[0m     delayed(optimize_model)(model_name, X_train, y_train, param_grids[model_name])\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m top_models\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m param_grids\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert results to dictionary\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimized_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(results)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting parallel optimization...\")\n",
    "overall_start = time.time()\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(optimize_model)(model_name, X_train, y_train, param_grids[model_name])\n",
    "    for model_name in top_models\n",
    "    if model_name in param_grids\n",
    ")\n",
    "\n",
    "# Convert results to dictionary\n",
    "optimized_models = dict(results)\n",
    "\n",
    "overall_time = time.time() - overall_start\n",
    "print(f\"\\nTotal optimization time: {overall_time:.2f} seconds\")\n",
    "print(f\"Successfully optimized {len(optimized_models)} models\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nOptimization Summary:\")\n",
    "print(\"-\" * 60)\n",
    "for model_name, result in optimized_models.items():\n",
    "    print(f\"{model_name}: CV Score = {result['cv_score']:.4f}, Time = {result['time_taken']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "942dc214-eae4-43f9-81b9-c77a7d801dd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimize each model one by one\n",
    "# for model_name in top_models:\n",
    "#     if model_name in param_grids:\n",
    "#         print(f\"\\nOptimizing {model_name}...\")\n",
    "        \n",
    "#         # Get base model\n",
    "#         if model_name == 'Random Forest':\n",
    "#             base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "#         elif model_name == 'Gradient Boosting':\n",
    "#             base_model = GradientBoostingClassifier(random_state=42)\n",
    "#         elif model_name == 'XGBoost':\n",
    "#             base_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "#         elif model_name == 'LightGBM':\n",
    "#             base_model = LGBMClassifier(random_state=42, verbosity=-1, is_unbalance=True)\n",
    "#         elif model_name == 'Neural Network':\n",
    "#             base_model = MLPClassifier(max_iter=1000, random_state=42)\n",
    "        \n",
    "#         # Grid search with cross-validation\n",
    "#         grid_search = GridSearchCV(\n",
    "#             base_model,\n",
    "#             param_grids[model_name],\n",
    "#             cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "#             scoring='roc_auc',\n",
    "#             n_jobs=-1,\n",
    "#             verbose=1\n",
    "#         )\n",
    "        \n",
    "#         # Fit grid search\n",
    "#         grid_search.fit(X_train, y_train)\n",
    "        \n",
    "#         # Store best model\n",
    "#         optimized_models[model_name] = {\n",
    "#             'model': grid_search.best_estimator_,\n",
    "#             'best_params': grid_search.best_params_,\n",
    "#             'cv_score': grid_search.best_score_\n",
    "#         }\n",
    "        \n",
    "#         print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "#         print(f\"Best CV ROC-AUC: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7770ec-c786-464a-bbc7-5ece5faed379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
